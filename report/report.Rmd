---
title: |
       | Constructing Bipartite Backbones
       | in Wireless Sensory Networks
author: "Will Spurgin^[Masters Candidate, Department of Computer Science,
Sourthern Methodist University. Email address:
[wspurgin@smu.edu](mailto:wspurgin@smu.edu).]"
abstract: |
  TODO FILL IN ABSTRACT
bibliography: report.bib
csl: elsevier-vancouver.csl
output:
  pdf_document:
    citation_package: biblatex
    keep_tex: true
    fig_caption: yes
    number_sections: yes
    template: template.latex
---

# Executive Summary

## Introduction

Imagine a scenario in which you are part of a team monitoring the volcano Mount
Pinatubo for surface temperature and seismic activity. Further, you are given a
multitude of low cost short-range wireless sensors to place around an area of
the volcano to take these measurements. Since Mt. Pinatubo is somewhat hard to
reach, you decide to drop these sensors from a helicopter. After dropping your
sensors over the area in a uniformly random fashion, you now need to read all
the data from these sensors.  However, since they are short-range wireless
sensors, you would have to be sufficiently close to a sensor to read its data.
However, since all the nodes are wireless, they can communicate their data to
each other. Ideally, you would rather go to one or relatively few nodes to
retrieve all the data. The question becomes: to which node(s) do you go?

This fictional scenario is the problem setting to which this report provides
solution discussion, reduction of those solution to practice, and a variety of
benchmark measurements. Formally, this report examines the construction of
backbones of communication in Wireless Sensor Networks (WSNs)
modeled as Random Geometric Graphs (RGGs). The results presented in this report
indicate that, generally, RGGs (and thus WSNs) with uniformly distributed nodes
can achieve near __100%__ coverage from a bipartite backbone using the methods
described in this report. The use of the methods presented in this report are
strongly recommended for any application involving WSNs or scenarios that can be
modeled as RGGs. Study of this work is not new[@mahjoub2010; @mahjoub2012], and
much of that work corroborates the findings of this report. The process for
choosing for the backbones in a WSN are straight forward:


1. For a graph $G$ (our RGG modeling a WSN), produce a Smallest Last
   Ordering[@matula1983] $S_{G}$.
2. For each vertex $v_j$ in $S_{G}$ color the vertex with the lowest available
   color class that is not a color of the neighbors of $v_j$ (i.e. $N(v_j)$).
3. For all 6 of the combination of the largest 4 color classes pick the top 2 by
   the number of edges of their maximum connected subgraph.
4. $\Delta$ These top two combinations are the two ideal backbones.

The implementation of these steps are further discussed in Section
$\ref{reduction-to-practice}$.

In the presentation of this report, several tables, and graphics are used to
make the seeing the merit of the results effortless. For all benchmarks, the
following plots are included: A plot of the RGG (with edges), a plot of the node
with lowest degree, a plot of the node with highest degree, a plot of the color
classes frequency, a plot of the degree distributions, a plot of the degree when
deleted versus original degree of the Smallest Last ordering algorithm, and
lastly the plot of the two major backbones of an RGG. Additionally, a table is
provided with summary information about the RGG (e.g. number of edges, vertices,
desired average degree, actual average degree, etc.). An abbreviated table of
results is given below in Table $\ref{tab:abb_res}$.

```{r include=FALSE}
library(knitr)
library(readr)
library(dplyr)

# Read in Generation Stats Data
gen_stat_files <- dir("../") %>% grep("*_gen_stats.csv", ., value=TRUE) %>%
    paste("..", ., sep="/")
backbone_stat_files <- dir("../") %>% grep("*_backbone_stats.csv", ., value=TRUE) %>%
    paste("..", ., sep="/")
num_stat_files <- length(gen_stat_files)
if (num_stat_files > 0) {
  gen_stats_df <- read_csv(gen_stat_files[1])
  if (num_stat_files > 1) {
    for (i in 2:num_stat_files) {
      gen_stats_df <- rbind(gen_stats_df, read_csv(gen_stat_files[i]))
    }
  }
}

num_bb_stat_files <- length(backbone_stat_files)
if (num_bb_stat_files > 0) {
  bb_stats_df <- read_csv(backbone_stat_files[1])
  if (num_bb_stat_files > 1) {
    for (i in 2:num_bb_stat_files) {
      bb_stats_df <- rbind(bb_stats_df, read_csv(backbone_stat_files[i]))
    }
  }
}

stats_df <- cbind(gen_stats_df, bb_stats_df)
stats_df$radius <- format(round(stats_df$radius,
                                               digits = 3), nsmall = 1)
stats_df$actual_average_degree <- format(round(stats_df$actual_average_degree,
                                               digits = 2), nsmall = 1)
stats_df$primary_backbone_coverage <-  paste(format(round(stats_df$primary_backbone_coverage * 100, digits = 2), nsmall = 1), "%", sep = "")

some_stats_df <- stats_df %>% select(one_of(colnames(gen_stats_df)),
                                     primary_backbone_coverage)

# Output data as a row table (since we have more vertical room).
some_stats_mat <- t(as.matrix(some_stats_df))
rownames(some_stats_mat) <- c("Number of Nodes", "Desired Avg. Degree", "Shape",
                       "Actual Avg. Degree", "Radius", "Number of Edges",
                       "Max Degree", "Min Degree", "Coverage")
colnames(some_stats_mat) <- NULL
```

```{r abb_res, echo=FALSE, results='asis'}
  kable(some_stats_mat, caption="Abbreviated Results", format = "latex", align="c")
```

As will be discussed further in Section $\ref{reduction-to-practice}$, the
algorithms used in this report are linear in the number of vertices and edges
${{\cal O}}(|V| + |E|)$[@matula1983; @mahjoub2010]. The implementations
presented in these reports make a few speed gains and memory reductions by using
efficient data structures. Figure $\ref{fig:linear_runtimes}$ shows the linear
runtime of the two phases of the project: RGG Generation and Bipartite Backbone
Construction (and Selection).

![Linear Runtimes\label{fig:linear_runtimes}](./figures/linear_runtimes.png)

## Environment Description

The results of this report were generated with the following hardware and
software:

- Hardware:
    - Apple MacBook Pro (Retina, Mid 2012)
    - Processor: 2.6 GHz Intel Core i7 (with turbo boost up to 3.3GHz)
    - Memory: 8 GB 1600 MHz DDR3
    - Intel HD Graphics 4000 1536 MB
    - Storage: 500.28 GB Solid State Drive, SATA Connection.
- Language: C++
    - Compilers:
      - GNU GCC 5.2.0 (Homebrew gcc 5.2.0 build)
      - Clang 8, Apple LLVM version 8.0.0 (clang-800.0.38)
- Graphical Tool:
    - R[@r2015] version 3.3.1 (2016-06-21) -- (codename "Bug in Your Hair")

All algorithms were written in `C++` using the `C++11` standard. These
implementation are less intensive on the resources of the system. The
utilization are the measured range of peak average utilization for each
benchmark.

- Resource Utilization:
    - CPU Utilization:  5% to 15%
    - Memory: 0.625 MB to 40 MB

The tool to generate the graphics for this report was R[@r2015]. R is a
statistical coding language, but has the benefit of a large community that has
built several packages for developing graphics. For the purposes of this
project, the core R libraries were the only graphical tools needed. However,
there are several packages for network analysis built for R[@statnet2008]. All
data that is given to R for displaying the various tables and graphs seen in
this report are passed via CSV files (as that format is easily read by R).

Further, R is used to generate this report into Latex (and then a PDF
thereafter). This document is written in R-Markdown[@rMarkdown2015] where the R
code is embedded to generate plots and tables.

# Reduction to Practice

In this section, the algorithms and implementations of those algorithms used in
this report are discussed in detail. To begin, the data structures need to model
the simulated data in this report as well as their relationships is presented.
Additionally, certain data structures are mentioned that enhance the algorithms
real runtime (complexity remains ${{\cal O}}(|V| + |E|)$). Thereafter, Section
$\ref{algorithm-description}$ describes the smallest last ordering
algorithm[@matula1983] and greedy graph coloring algorithm implemented in this
work. The subsequent section substantiates the linear runtime of the smallest
last ordering and coloring algorithm implementations. In Section
$\ref{walkthrough-and-verification}$ a full walkthrough from generation to
bipartite backbone selection is given. Lastly, a discussion of the effectiveness
of these algorithms is given in Section $\ref{algorithm-effectiveness}$.

## Data Structures

A graph is simple a set of **vertices** and a set of relations between those
vertices called **edges**. In our scenario, each vertex has geometric data as
well ($x$ and $y$ coordinates). This report took the simplest approach when
modeling

## Algorithm Description

## Algorithm Engineering

## Walkthrough and Verification

## Algorithm Effectiveness

# Simulated Benchmark Results

# Conclusion
Duis nec purus sed neque porttitor tincidunt vitae quis augue. Donec porttitor
aliquam ante, nec convallis nisl ornare eu. Morbi ut purus et justo commodo
dignissim et nec nisl. Donec imperdiet tellus dolor, vel dignissim risus
venenatis eu. Aliquam tempor imperdiet massa, nec fermentum tellus sollicitudin
vulputate. Integer posuere porttitor pharetra. Praesent vehicula elementum diam
a suscipit. Morbi viverra velit eget placerat pellentesque. Nunc congue augue
non nisi ultrices tempor.

# References {#references}

